# ==============================================================================
# ARWKV REPLICATION: PHASE 1 (Attention Alignment) - FIXED v3
# ==============================================================================
# Fixes:
# 1. Correctly handles Qwen2Attention return tuple (2 items instead of 3).
# 2. Implements JIT layer-wise distillation without storing full-graph activations.
# 3. Includes Manual Causal Mask expansion.
# ==============================================================================

import subprocess
import sys
import os

# --- 1. SETUP & INSTALLATION ---
def install_requirements():
    required = ["transformers", "accelerate", "deepspeed", "datasets", "tiktoken"]
    try:
        import transformers
        import deepspeed
        import datasets
    except ImportError:
        print("Installing dependencies...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q"] + required)
        print("Dependencies installed.")

install_requirements()

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torch.optim as optim
import triton
import triton.language as tl
from transformers import Qwen2ForCausalLM, AutoTokenizer, default_data_collator
from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask
from datasets import load_dataset
from tqdm.auto import tqdm

# ==============================================================================
# 2. MODEL ARCHITECTURE
# ==============================================================================

FLASHRNN_CHUNK = 128
FLASHRNN_BLOCK_D = 128


@triton.jit
def _flashrnn_chunk_kernel(
    w_ptr,
    k_ptr,
    v_ptr,
    r_ptr,
    s_ptr,
    out_ptr,
    stride_w_bh,
    stride_w_t,
    stride_w_d,
    stride_k_bh,
    stride_k_t,
    stride_k_d,
    stride_v_bh,
    stride_v_t,
    stride_v_d,
    stride_r_bh,
    stride_r_t,
    stride_r_d,
    stride_s_bh,
    stride_s_row,
    stride_s_col,
    stride_out_bh,
    stride_out_t,
    stride_out_d,
    chunk_start,
    chunk_len,
    D,
    BLOCK_D: tl.constexpr,
    CHUNK: tl.constexpr,
):
    pid = tl.program_id(0)

    d = tl.arange(0, BLOCK_D)
    rows = d[:, None]
    cols = d[None, :]
    dim_mask = d < D
    state_offsets = pid * stride_s_bh + rows * stride_s_row + cols * stride_s_col
    s = tl.load(s_ptr + state_offsets, mask=(rows < D) & (cols < D), other=0.0).to(
        tl.float32
    )

    for t in range(CHUNK):
        active = chunk_len > t

        base = chunk_start + t

        w_offset = pid * stride_w_bh + base * stride_w_t
        k_offset = pid * stride_k_bh + base * stride_k_t
        v_offset = pid * stride_v_bh + base * stride_v_t
        r_offset = pid * stride_r_bh + base * stride_r_t

        w_t = tl.load(
            w_ptr + w_offset + d, mask=dim_mask & active, other=1.0
        ).to(tl.float32)
        k_t = tl.load(
            k_ptr + k_offset + d, mask=dim_mask & active, other=0.0
        ).to(tl.float32)
        v_t = tl.load(
            v_ptr + v_offset + d, mask=dim_mask & active, other=0.0
        ).to(tl.float32)
        r_t = tl.load(
            r_ptr + r_offset + d, mask=dim_mask & active, other=0.0
        ).to(tl.float32)

        s = s * w_t[:, None] + v_t[:, None] * k_t[None, :]
        y = tl.sum(s * r_t[None, :], axis=1)

        out_offset = pid * stride_out_bh + base * stride_out_t
        tl.store(
    out_ptr + out_offset + d,
    y.to(tl.bfloat16),
    mask=dim_mask & active,)

    tl.store(
    s_ptr + state_offsets,
    s.to(tl.bfloat16), # Assuming state is stored in BF16
    mask=(rows < D) & (cols < D),
)


class RWKV7_TimeMix(nn.Module):
    """
    RWKV-7 'Goose' TimeMixing Module.
    Faithful implementation of Eq. 3 from the ARWKV paper.
    """
    def __init__(self, layer_id, n_embd, n_head, head_size):
        super().__init__()
        self.layer_id = layer_id
        self.n_head = n_head
        self.head_size = head_size
        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))

        # RWKV-7 Parameters
        self.x_r = nn.Parameter(torch.zeros(n_embd))
        self.x_w = nn.Parameter(torch.zeros(n_embd))
        self.x_k = nn.Parameter(torch.zeros(n_embd))
        self.x_v = nn.Parameter(torch.zeros(n_embd))
        self.x_a = nn.Parameter(torch.zeros(n_embd))
        self.x_g = nn.Parameter(torch.zeros(n_embd))

        self.r_proj = nn.Linear(n_embd, n_embd, bias=False)
        self.k_proj = nn.Linear(n_embd, n_embd, bias=False)
        self.v_proj = nn.Linear(n_embd, n_embd, bias=False)
        self.o_proj = nn.Linear(n_embd, n_embd, bias=False)
        self.g_proj = nn.Linear(n_embd, n_embd, bias=False)
        self.w_proj = nn.Linear(n_embd, n_embd, bias=False)
        self.a_proj = nn.Linear(n_embd, n_embd, bias=False)

        self.ln_x = nn.GroupNorm(n_head, n_embd, eps=1e-5)

    def forward(self, x, state=None):
        B, T, C = x.size()
        H = self.n_head

        xx = self.time_shift(x)
        if state is not None and 'prev_x' in state:
            xx[:, 0, :] = state['prev_x']

        xr = x + self.x_r * (xx - x)
        xw = x + self.x_w * (xx - x)
        xk = x + self.x_k * (xx - x)
        xv = x + self.x_v * (xx - x)
        xa = x + self.x_a * (xx - x)
        xg = x + self.x_g * (xx - x)

        r = self.r_proj(xr)
        w = -torch.exp(self.w_proj(xw))
        k = self.k_proj(xk)
        v = self.v_proj(xv)
        a = torch.sigmoid(self.a_proj(xa))
        g = torch.sigmoid(self.g_proj(xg))

        r = r.view(B, T, H, -1)
        w = w.view(B, T, H, -1)
        k = k.view(B, T, H, -1)
        v = v.view(B, T, H, -1)
        a = a.view(B, T, H, -1)

        w = torch.exp(w)

        if state is None or 's' not in state:
            s = torch.zeros(B, H, self.head_size, self.head_size, device=x.device, dtype=x.dtype)
        else:
            s = state['s']

        x_out, s = self.flashrnn_time_mix(r, w, k, v, s)
        x_out = x_out.reshape(B, T, C)
        x_out = self.ln_x(x_out.transpose(1, 2)).transpose(1, 2)
        x_out = x_out * g
        x_out = self.o_proj(x_out)

        return x_out, {'prev_x': x[:, -1, :], 's': s}

    def flashrnn_time_mix(self, r, w, k, v, s):
        B, T, H, D = r.shape
        chunk = FLASHRNN_CHUNK
        bh = B * H

        r_flat = r.reshape(bh, T, D).contiguous()
        w_flat = w.reshape(bh, T, D).contiguous()
        k_flat = k.reshape(bh, T, D).contiguous()
        v_flat = v.reshape(bh, T, D).contiguous()
        out_flat = torch.empty_like(r_flat)

        s_flat = s.reshape(bh, D, D).contiguous()

        grid = (bh,)
        for start in range(0, T, chunk):
            chunk_len = min(chunk, T - start)
            _flashrnn_chunk_kernel[grid](
                w_flat,
                k_flat,
                v_flat,
                r_flat,
                s_flat,
                out_flat,
                w_flat.stride(0),
                w_flat.stride(1),
                w_flat.stride(2),
                k_flat.stride(0),
                k_flat.stride(1),
                k_flat.stride(2),
                v_flat.stride(0),
                v_flat.stride(1),
                v_flat.stride(2),
                r_flat.stride(0),
                r_flat.stride(1),
                r_flat.stride(2),
                s_flat.stride(0),
                s_flat.stride(1),
                s_flat.stride(2),
                out_flat.stride(0),
                out_flat.stride(1),
                out_flat.stride(2),
                start,
                chunk_len,
                D,
                BLOCK_D=FLASHRNN_BLOCK_D,
                CHUNK=chunk,
            )

        return out_flat.reshape(B, T, H, D), s_flat.reshape(B, H, D, D)

class AttentionWrapper(nn.Module):
    """
    Updated Wrapper that accepts explicit rotary position ids and handles Qwen2 return type correctly.
    """
    def __init__(self, qwen_layer, rwkv_layer, d_model):
        super().__init__()
        self.teacher_attn = qwen_layer.self_attn
        self.teacher_norm = qwen_layer.input_layernorm
        self.student_mixer = rwkv_layer
        self.d_model = d_model

        for p in self.teacher_attn.parameters():
            p.requires_grad = False
        for p in self.teacher_norm.parameters():
            p.requires_grad = False

    def forward(self, hidden_states, attention_mask=None, position_ids=None, cache_position=None):
        normed_states = self.teacher_norm(hidden_states)

        with torch.no_grad():
            # FIXED: explicitly take the first element [0]
            # Qwen2Attention returns (hidden_states, past_key_values)
            teacher_out = self.teacher_attn(
                hidden_states=normed_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                cache_position=cache_position
            )[0]

        student_out, _ = self.student_mixer(normed_states)
        diff = teacher_out - student_out
        loss = torch.norm(diff, p=2, dim=-1).mean() * (self.d_model ** -0.5)
        return loss, student_out

# ==============================================================================
# 3. PIPELINE BUILDER & DATA
# ==============================================================================

RWKV_HEAD_SIZE = 64

def build_arwkv_stage1_model(model_name="Qwen/Qwen2.5-1.5B-Instruct"):
    print(f"Loading Teacher Model: {model_name}")
    teacher_model = Qwen2ForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map="cpu",
        attn_implementation="eager"
    )
    config = teacher_model.config

    wrappers = nn.ModuleList()

    print("Grafting RWKV-7 Modules...")
    for i, layer in enumerate(teacher_model.model.layers):
        n_head_rwkv = config.hidden_size // RWKV_HEAD_SIZE
        rwkv_layer = RWKV7_TimeMix(
            layer_id=i,
            n_embd=config.hidden_size,
            n_head=n_head_rwkv,
            head_size=RWKV_HEAD_SIZE
        )
        rwkv_layer.to(torch.bfloat16)
        wrapper = AttentionWrapper(layer, rwkv_layer, config.hidden_size)
        wrappers.append(wrapper)

    print(f"Pipeline Ready: {len(wrappers)} layers.")
    return teacher_model, wrappers

def get_dataloader(tokenizer, batch_size=1):
    print("Initializing Data Stream...")
    try:
        dataset = load_dataset(
            "HuggingFaceFW/fineweb-edu",
            name="sample-10BT",
            split="train",
            streaming=True,
            storage_options={'timeout': 1200}
        )
        print("Success: Streaming FineWeb-Edu.")
        is_streaming = True
    except Exception as e:
        print(f"Warning: FineWeb failed ({str(e)}). Fallback to WikiText-2.")
        dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
        is_streaming = False

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=2048,
            padding="max_length"
        )

    def filter_short(x):
        return len(x["text"]) > 100

    if not is_streaming:
        cols_to_remove = [c for c in dataset.column_names if c != 'text']
        tokenized_dataset = dataset.filter(filter_short).map(
            tokenize_function,
            batched=True,
            remove_columns=cols_to_remove
        )
    else:
        tokenized_dataset = dataset.filter(filter_short).map(
            tokenize_function,
            batched=True,
            remove_columns=["text", "id", "dump", "url", "file_path", "language", "language_score", "token_count", "score", "int_score"]
        )

    return DataLoader(
        tokenized_dataset,
        batch_size=batch_size,
        collate_fn=default_data_collator
    )

# ==============================================================================
# 4. TRAINING EXECUTION
# ==============================================================================

def train_stage_1(teacher_model, wrappers, dataloader, device="cuda", steps=100):
    print(f"Starting Stage 1 Alignment on {device}...")
    teacher_model.to(device)
    teacher_model.eval()

    student_params = []
    for w in wrappers:
        w.to(device)
        student_params.extend(w.student_mixer.parameters())

    optimizer = optim.AdamW(student_params, lr=1e-3, betas=(0.9, 0.99))
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps * len(wrappers))

    progress_bar = tqdm(range(steps))
    data_iter = iter(dataloader)

    for step in progress_bar:
        try:
            batch = next(data_iter)
        except StopIteration:
            data_iter = iter(dataloader)
            batch = next(data_iter)

        input_ids = batch['input_ids'].to(device)
        raw_mask = batch['attention_mask'].to(device)

        with torch.no_grad():
            inputs_embeds = teacher_model.model.embed_tokens(input_ids)
            expanded_mask = _prepare_4d_causal_attention_mask(
                attention_mask=raw_mask,
                input_shape=(input_ids.shape[0], input_ids.shape[1]),
                inputs_embeds=inputs_embeds,
                past_key_values_length=0,
            )
            seq_len = input_ids.shape[1]
            cache_position = torch.arange(seq_len, device=device)
            position_ids = cache_position.unsqueeze(0).expand(input_ids.shape[0], -1)
            teacher_hidden = inputs_embeds

        for layer_idx, wrapper in enumerate(wrappers):
            optimizer.zero_grad()

            teacher_input = teacher_hidden

            with torch.no_grad():
                teacher_layer = teacher_model.model.layers[layer_idx]
                teacher_output = teacher_layer(
                    teacher_input,
                    attention_mask=expanded_mask,
                    position_ids=position_ids,
                    output_attentions=False,
                    use_cache=False,
                    cache_position=cache_position,
                )
                teacher_hidden = teacher_output[0]

            loss, _ = wrapper(
                teacher_input,
                attention_mask=expanded_mask,
                position_ids=position_ids,
                cache_position=cache_position
            )

            loss.backward()

            torch.nn.utils.clip_grad_norm_(wrapper.student_mixer.parameters(), 1.0)
            optimizer.step()
            scheduler.step()

            progress_bar.set_description(f"Layer {layer_idx} Loss: {loss.item():.4f}")

            del teacher_output, loss
            if device.startswith("cuda"):
                torch.cuda.empty_cache()

    print("Stage 1 Training Complete.")
    return wrappers

if __name__ == "__main__":
    torch.cuda.empty_cache()
    model_name = "Qwen/Qwen2.5-1.5B-Instruct"
    print("Initializing Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    teacher, wrappers = build_arwkv_stage1_model(model_name)
    loader = get_dataloader(tokenizer, batch_size=1)

    train_stage_1(teacher, wrappers, loader, device="cuda", steps=10)


# ==============================================================================
# ARWKV REPLICATION: PHASE 2 (Knowledge Distillation) - FIXED
# ==============================================================================
# Fixes: RWKV_Shim return signature adapted to match Qwen2DecoderLayer expectation.
# ==============================================================================

import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from tqdm.auto import tqdm

# --- 1. THE ADAPTER SHIM (FIXED) ---
class RWKV_Shim(nn.Module):
    """
    Acts as a mechanical adapter.
    It sits in the 'self_attn' slot of a Qwen2DecoderLayer.
    """
    def __init__(self, rwkv_module):
        super().__init__()
        self.rwkv = rwkv_module

    def forward(
        self,
        hidden_states,
        attention_mask=None,
        position_ids=None,
        past_key_values=None,
        output_attentions=False,
        use_cache=False,
        position_embeddings=None,
        **kwargs
    ):
        # 1. Run RWKV TimeMix
        # We ignore position_embeddings (RoPE) because RWKV tracks state internally.
        rwkv_out, new_state = self.rwkv(hidden_states)

        # 2. Return format expected by Qwen2DecoderLayer
        # traceback indicated it expects exactly 2 items: (hidden_states, past_key_values)
        return (rwkv_out, None)

# --- 2. MODEL CONVERSION ---
def create_student_model(teacher_model, stage1_wrappers):
    print("Cloning Teacher to create Student Base...")
    # Deep copy the teacher to preserve the original for distillation
    student_model = copy.deepcopy(teacher_model)

    print("Surgically replacing Attention with RWKV-7...")
    for i, layer in enumerate(student_model.model.layers):
        # Extract the trained RWKV module from Stage 1 wrapper
        trained_rwkv = stage1_wrappers[i].student_mixer

        # Create the shim
        shim = RWKV_Shim(trained_rwkv)

        # Replace the Attention layer
        layer.self_attn = shim

    print("Student Model Architecture Updated.")
    return student_model

# --- 3. STAGE 2 TRAINING LOOP ---
def train_stage_2(teacher_model, student_model, dataloader, device="cuda", steps=100):
    print(f"Starting Stage 2 (Knowledge Distillation) on {device}...")

    # 1. Setup Models
    teacher_model.to(device)
    teacher_model.eval() # Teacher is always frozen
    for p in teacher_model.parameters():
        p.requires_grad = False

    student_model.to(device)
    student_model.train()

    # 2. Configure "Active MLP" Training
    # We unfreeze all parameters in the student to allow MLPs to adapt
    trainable_params = []
    for name, param in student_model.named_parameters():
        param.requires_grad = True
        trainable_params.append(param)

    print(f"Training {len(trainable_params)} tensor groups (Full Student Update).")

    # 3. Optimization
    optimizer = optim.AdamW(trainable_params, lr=5e-5, betas=(0.9, 0.99))
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps)

    # 4. Loss Function: KL Divergence
    loss_fct = nn.KLDivLoss(reduction="batchmean")

    progress_bar = tqdm(range(steps))
    data_iter = iter(dataloader)

    for step in progress_bar:
        try:
            batch = next(data_iter)
        except StopIteration:
            data_iter = iter(dataloader)
            batch = next(data_iter)

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)

        optimizer.zero_grad()

        # A. Get Teacher Logits (Target)
        with torch.no_grad():
            teacher_outputs = teacher_model(input_ids, attention_mask=attention_mask)
            teacher_logits = teacher_outputs.logits

        # B. Get Student Logits (Prediction)
        student_outputs = student_model(input_ids, attention_mask=attention_mask)
        student_logits = student_outputs.logits

        # C. Compute KL Divergence Loss
        # Input: LogSoftmax(Student), Target: Softmax(Teacher)
        loss = loss_fct(
            F.log_softmax(student_logits, dim=-1),
            F.softmax(teacher_logits, dim=-1)
        )

        # D. Backward
        loss.backward()
        torch.nn.utils.clip_grad_norm_(trainable_params, 1.0)
        optimizer.step()
        scheduler.step()

        progress_bar.set_description(f"KD Loss: {loss.item():.4f}")

    print("Stage 2 Training Complete.")
    return student_model

# --- 4. EXECUTION ---
if __name__ == "__main__":
    torch.cuda.empty_cache()

    # Verification check
    if 'teacher' not in globals() or 'wrappers' not in globals():
        print("Error: Phase 1 variables not found. Please run Phase 1 first.")
    else:
        # Create Student
        student = create_student_model(teacher, wrappers)

        # Run Distillation
        # Note: loader comes from Phase 1 variables
        trained_student = train_stage_2(teacher, student, loader, device="cuda", steps=10)

# ==============================================================================
# ARWKV REPLICATION: PHASE 3 (SFT & DPO)
# ==============================================================================
# Objective:
# 1. SFT: Adapt the RNN-Hybrid for instruction following.
# 2. DPO: Align preferences using the SFT model as a reference.
# ==============================================================================

import torch
import torch.nn.functional as F
import copy
from torch.utils.data import DataLoader, Dataset
from transformers import default_data_collator

# ==============================================================================
# 1. SETUP & DATASETS (Using small proxies for demonstration)
# ==============================================================================

# We use a dummy dataset generator to emulate SFT/DPO data structures
# to ensure this code runs immediately without downloading massive instruction sets.
class DummyInstructionDataset(Dataset):
    def __init__(self, tokenizer, length=100):
        self.tokenizer = tokenizer
        self.length = length

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        # Format: User instruction -> Assistant response
        text = f"<|im_start|>user\nExplain RWKV.<|im_end|>\n<|im_start|>assistant\nRWKV is an RNN with Transformer-level performance.<|im_end|>"
        return self.tokenizer(
            text,
            truncation=True,
            max_length=512,
            padding="max_length",
            return_tensors=None # Let collator handle tensors
        )

class DummyPreferenceDataset(Dataset):
    def __init__(self, tokenizer, length=100):
        self.tokenizer = tokenizer
        self.length = length

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        prompt = "Explain RWKV."
        chosen = "RWKV is an efficient RNN."
        rejected = "RWKV is a type of ice cream."

        # DPO needs separate encodings
        def enc(p, r):
            return self.tokenizer(
                f"<|im_start|>user\n{p}<|im_end|>\n<|im_start|>assistant\n{r}<|im_end|>",
                truncation=True, max_length=512, padding="max_length"
            )

        return {
            "chosen_input_ids": enc(prompt, chosen)["input_ids"],
            "chosen_attention_mask": enc(prompt, chosen)["attention_mask"],
            "rejected_input_ids": enc(prompt, rejected)["input_ids"],
            "rejected_attention_mask": enc(prompt, rejected)["attention_mask"],
        }

# ==============================================================================
# 2. SFT TRAINING LOOP (Next Token Prediction)
# ==============================================================================
def train_stage_3_sft(model, tokenizer, device="cuda", steps=10):
    print("\n--- Starting Stage 3a: Supervised Fine-Tuning (SFT) ---")

    # Dataset
    train_dataset = DummyInstructionDataset(tokenizer)
    dataloader = DataLoader(train_dataset, batch_size=1, collate_fn=default_data_collator)

    # Optimizer (Paper uses lower LR for fine-tuning)
    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)

    progress_bar = tqdm(range(steps))
    data_iter = iter(dataloader)

    for step in progress_bar:
        try:
            batch = next(data_iter)
        except StopIteration:
            data_iter = iter(dataloader)
            batch = next(data_iter)

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = input_ids.clone() # Standard Causal LM training

        optimizer.zero_grad()

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss

        loss.backward()
        optimizer.step()

        progress_bar.set_description(f"SFT Loss: {loss.item():.4f}")

    print("SFT Complete. Model context adapted.")
    return model

# ==============================================================================
# 3. DPO TRAINING LOOP (Direct Preference Optimization)
# ==============================================================================
def train_stage_3_dpo(model, tokenizer, device="cuda", steps=10, beta=0.1):
    print("\n--- Starting Stage 3b: Direct Preference Optimization (DPO) ---")

    # 1. Prepare Reference Model (Frozen Copy of SFT Model)
    # DPO requires a reference to calculate the ratio of probabilities
    ref_model = copy.deepcopy(model)
    ref_model.eval()
    ref_model.to(device)
    for p in ref_model.parameters():
        p.requires_grad = False

    # 2. Dataset
    # Custom collator for DPO pairs
    def dpo_collate(batch):
        return {
            "chosen_input_ids": torch.tensor([b["chosen_input_ids"] for b in batch]),
            "chosen_attention_mask": torch.tensor([b["chosen_attention_mask"] for b in batch]),
            "rejected_input_ids": torch.tensor([b["rejected_input_ids"] for b in batch]),
            "rejected_attention_mask": torch.tensor([b["rejected_attention_mask"] for b in batch]),
        }

    train_dataset = DummyPreferenceDataset(tokenizer)
    dataloader = DataLoader(train_dataset, batch_size=1, collate_fn=dpo_collate)

    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6) # DPO requires very low LR

    progress_bar = tqdm(range(steps))
    data_iter = iter(dataloader)

    for step in progress_bar:
        try:
            batch = next(data_iter)
        except StopIteration:
            data_iter = iter(dataloader)
            batch = next(data_iter)

        # Move to device
        c_ids = batch['chosen_input_ids'].to(device)
        c_mask = batch['chosen_attention_mask'].to(device)
        r_ids = batch['rejected_input_ids'].to(device)
        r_mask = batch['rejected_attention_mask'].to(device)

        optimizer.zero_grad()

        # --- DPO LOGIC ---

        # Helper to get log probabilities of the completion segments
        def get_log_probs(m, ids, mask):
            outputs = m(ids, attention_mask=mask)
            logits = outputs.logits[:, :-1, :]
            labels = ids[:, 1:]

            # log_softmax
            log_probs = F.log_softmax(logits, dim=-1)

            # gather the log probs of the actual tokens
            selected_log_probs = torch.gather(log_probs, -1, labels.unsqueeze(-1)).squeeze(-1)

            # Mask out padding (so we don't count padding tokens in the score)
            # We assume mask is 1 for real tokens. shift mask to match labels
            mask_shifted = mask[:, 1:]
            selected_log_probs = selected_log_probs * mask_shifted

            # Sum per sequence
            return selected_log_probs.sum(dim=-1)

        # 1. Policy LogProbs
        policy_chosen_logps = get_log_probs(model, c_ids, c_mask)
        policy_rejected_logps = get_log_probs(model, r_ids, r_mask)

        # 2. Reference LogProbs (No Grad)
        with torch.no_grad():
            ref_chosen_logps = get_log_probs(ref_model, c_ids, c_mask)
            ref_rejected_logps = get_log_probs(ref_model, r_ids, r_mask)

        # 3. DPO Loss Calculation (Eq. 7 in DPO paper)
        # L_DPO = -log(sigmoid(beta * (log(pi_c/ref_c) - log(pi_r/ref_r))))

        chosen_ratio = policy_chosen_logps - ref_chosen_logps
        rejected_ratio = policy_rejected_logps - ref_rejected_logps

        preference_logits = beta * (chosen_ratio - rejected_ratio)
        dpo_loss = -F.logsigmoid(preference_logits).mean()

        dpo_loss.backward()
        optimizer.step()

        progress_bar.set_description(f"DPO Loss: {dpo_loss.item():.4f}")

    print("Stage 3 (SFT+DPO) Complete. ARWKV Replication Finished.")
    return model

# ==============================================================================
# 4. EXECUTION
# ==============================================================================
if __name__ == "__main__":
    torch.cuda.empty_cache()

    # Ensure Stage 2 trained model exists
    if 'trained_student' in globals():
        # Run Stage 3a: SFT
        sft_model = train_stage_3_sft(trained_student, tokenizer, device="cuda", steps=10)

        # Run Stage 3b: DPO
        final_model = train_stage_3_dpo(sft_model, tokenizer, device="cuda", steps=10)

        print("\n=== REPLICATION COMPLETE ===")
        print("The 'final_model' object is now the fully distilled ARWKV-7 1.5B RNN.")
    else:
        print("Error: Stage 2 model 'trained_student' not found. Please run Phase 2.")

# ==============================================================================
# ARWKV REPLICATION: INFERENCE TEST
# ==============================================================================

import torch
from transformers import TextStreamer

def test_arwkv_model(model, tokenizer, prompt_text):
    print(f"\n--- Testing ARWKV Model ---")

    model.eval()
    model.to("cuda")

    # 1. Format Prompt (Qwen 2.5 Chat Template)
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": prompt_text}
    ]

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    print(f"INPUT PROMPT:\n{text}\n" + "-"*40)

    model_inputs = tokenizer([text], return_tensors="pt").to("cuda")

    # 2. Generation
    # CRITICAL: use_cache=False.
    # Since our Shim implementation calculates state from scratch based on input_ids,
    # we must feed the full sequence every step (Quadratic/Transformer-style inference).
    # To get linear RNN inference, we would need to export the state explicitly.

    # Using a Streamer for real-time output
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    with torch.no_grad():
        generated_ids = model.generate(
            model_inputs.input_ids,
            attention_mask=model_inputs.attention_mask,
            max_new_tokens=100,
            use_cache=False,       # Required for this shim implementation
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            streamer=streamer,
            pad_token_id=tokenizer.eos_token_id
        )

    print("\n" + "-"*40 + "\nGeneration Complete.")

# --- Run the Test ---
# We test with a simple prompt.
# Note: Since Stage 2 was only 10 steps, the model may be incoherent.
# A fully converged model requires ~20M tokens in Stage 2.
if 'final_model' in globals():
    test_arwkv_model(final_model, tokenizer, "Explain the difference between a Transformer and an RNN.")
else:
    print("Error: 'final_model' not found. Please complete Stage 3 first.")
