# ==============================================================================
# ARWKV REPLICATION: COMPLETE HARNESS (v6 - Hook-Based Distillation)
# ==============================================================================
# Fixes Applied:
# 1. RoPE/Shape Error: Removed manual teacher simulation. Now uses Hooks to capture
#    the exact Teacher output during a native forward pass.
# 2. Performance: 2x faster Stage 1 (Teacher Attn is computed once, not twice).
# 3. Triton Fix: bf16 type casting preserved.
# 4. Initialization: Zero-Init preserved.
# ==============================================================================

import subprocess
import sys
import os
import copy
import math

# --- 1. SETUP & INSTALLATION ---
def install_requirements():
    required = ["transformers", "accelerate", "deepspeed", "datasets", "tiktoken", "triton"]
    try:
        import transformers
        import deepspeed
        import datasets
        import triton
    except ImportError:
        print("Installing dependencies...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q"] + required)
        print("Dependencies installed.")

install_requirements()

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import triton
import triton.language as tl
from torch.utils.data import DataLoader
from transformers import Qwen2ForCausalLM, AutoTokenizer, default_data_collator, TextStreamer
from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask
from datasets import load_dataset
from tqdm.auto import tqdm

# ==============================================================================
# 2. TRITON KERNEL
# ==============================================================================
FLASHRNN_CHUNK = 128
FLASHRNN_BLOCK_D = 128

@triton.jit
def _flashrnn_chunk_kernel(
    w_ptr, k_ptr, v_ptr, r_ptr, s_ptr, out_ptr,
    stride_w_bh, stride_w_t, stride_w_d,
    stride_k_bh, stride_k_t, stride_k_d,
    stride_v_bh, stride_v_t, stride_v_d,
    stride_r_bh, stride_r_t, stride_r_d,
    stride_s_bh, stride_s_row, stride_s_col,
    stride_out_bh, stride_out_t, stride_out_d,
    chunk_start, chunk_len,
    D,
    BLOCK_D: tl.constexpr,
    CHUNK: tl.constexpr,
):
    pid = tl.program_id(0)
    d = tl.arange(0, BLOCK_D)
    rows = d[:, None]
    cols = d[None, :]
    dim_mask = d < D

    state_offsets = pid * stride_s_bh + rows * stride_s_row + cols * stride_s_col
    s = tl.load(s_ptr + state_offsets, mask=(rows < D) & (cols < D), other=0.0).to(tl.float32)

    for t in range(CHUNK):
        active = chunk_len > t
        base = chunk_start + t
        
        w_offset = pid * stride_w_bh + base * stride_w_t
        k_offset = pid * stride_k_bh + base * stride_k_t
        v_offset = pid * stride_v_bh + base * stride_v_t
        r_offset = pid * stride_r_bh + base * stride_r_t
        
        w_t = tl.load(w_ptr + w_offset + d, mask=dim_mask & active, other=1.0).to(tl.float32)
        k_t = tl.load(k_ptr + k_offset + d, mask=dim_mask & active, other=0.0).to(tl.float32)
        v_t = tl.load(v_ptr + v_offset + d, mask=dim_mask & active, other=0.0).to(tl.float32)
        r_t = tl.load(r_ptr + r_offset + d, mask=dim_mask & active, other=0.0).to(tl.float32)

        s = s * w_t[:, None] + v_t[:, None] * k_t[None, :]
        y = tl.sum(s * r_t[None, :], axis=1)

        out_offset = pid * stride_out_bh + base * stride_out_t
        tl.store(out_ptr + out_offset + d, y.to(tl.bfloat16), mask=dim_mask & active)

    tl.store(s_ptr + state_offsets, s.to(tl.bfloat16), mask=(rows < D) & (cols < D))

# ==============================================================================
# 3. MODEL ARCHITECTURE
# ==============================================================================
class RWKV7_TimeMix(nn.Module):
    def __init__(self, layer_id, n_embd, n_head, head_size):
        super().__init__()
        self.layer_id = layer_id
        self.n_head = n_head
        self.head_size = head_size
        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))

        self.x_r = nn.Parameter(torch.zeros(n_embd))
        self.x_w = nn.Parameter(torch.zeros(n_embd))
        self.x_k = nn.Parameter(torch.zeros(n_embd))
        self.x_v = nn.Parameter(torch.zeros(n_embd))
        self.x_a = nn.Parameter(torch.zeros(n_embd))
        self.x_g = nn.Parameter(torch.zeros(n_embd))

        self.r_proj = nn.Linear(n_embd, n_embd, bias=False)
        self.k_proj = nn.Linear(n_embd, n_embd, bias=False)
        self.v_proj = nn.Linear(n_embd, n_embd, bias=False)
        self.o_proj = nn.Linear(n_embd, n_embd, bias=False)
        self.g_proj = nn.Linear(n_embd, n_embd, bias=False)
        self.w_proj = nn.Linear(n_embd, n_embd, bias=False)
        self.a_proj = nn.Linear(n_embd, n_embd, bias=False)

        self.ln_x = nn.GroupNorm(n_head, n_embd, eps=1e-5)

        # Zero-Init
        nn.init.orthogonal_(self.r_proj.weight, gain=0.1)
        nn.init.orthogonal_(self.k_proj.weight, gain=0.1)
        nn.init.orthogonal_(self.v_proj.weight, gain=0.1)
        nn.init.orthogonal_(self.g_proj.weight, gain=0.1)
        nn.init.constant_(self.w_proj.weight, -3.0)
        nn.init.zeros_(self.o_proj.weight)

    def forward(self, x, state=None):
        B, T, C = x.size()
        H = self.n_head
        
        xx = self.time_shift(x)
        if state is not None and 'prev_x' in state: xx[:, 0, :] = state['prev_x']

        xr = x + self.x_r * (xx - x)
        xw = x + self.x_w * (xx - x)
        xk = x + self.x_k * (xx - x)
        xv = x + self.x_v * (xx - x)
        xa = x + self.x_a * (xx - x)
        xg = x + self.x_g * (xx - x)

        r = self.r_proj(xr); w = -torch.exp(self.w_proj(xw))
        k = self.k_proj(xk); v = self.v_proj(xv)
        a = torch.sigmoid(self.a_proj(xa)); g = torch.sigmoid(self.g_proj(xg))

        r = r.view(B, T, H, -1); w = w.view(B, T, H, -1)
        k = k.view(B, T, H, -1); v = v.view(B, T, H, -1)
        w = torch.exp(w)

        if state is None or 's' not in state:
            s = torch.zeros(B, H, self.head_size, self.head_size, device=x.device, dtype=x.dtype)
        else: s = state['s']

        x_out, s = self.flashrnn_time_mix(r, w, k, v, s)

        x_out = x_out.reshape(B, T, C)
        x_out = self.ln_x(x_out.transpose(1, 2)).transpose(1, 2)
        x_out = x_out * g
        x_out = self.o_proj(x_out)

        return x_out, {'prev_x': x[:, -1, :], 's': s}

    def flashrnn_time_mix(self, r, w, k, v, s):
        B, T, H, D = r.shape
        chunk = FLASHRNN_CHUNK
        bh = B * H

        r_flat = r.reshape(bh, T, D).contiguous()
        w_flat = w.reshape(bh, T, D).contiguous()
        k_flat = k.reshape(bh, T, D).contiguous()
        v_flat = v.reshape(bh, T, D).contiguous()
        out_flat = torch.empty_like(r_flat)
        s_flat = s.reshape(bh, D, D).contiguous()

        grid = (bh,)
        for start in range(0, T, chunk):
            chunk_len = min(chunk, T - start)
            _flashrnn_chunk_kernel[grid](
                w_flat, k_flat, v_flat, r_flat, s_flat, out_flat,
                w_flat.stride(0), w_flat.stride(1), w_flat.stride(2),
                k_flat.stride(0), k_flat.stride(1), k_flat.stride(2),
                v_flat.stride(0), v_flat.stride(1), v_flat.stride(2),
                r_flat.stride(0), r_flat.stride(1), r_flat.stride(2),
                s_flat.stride(0), s_flat.stride(1), s_flat.stride(2),
                out_flat.stride(0), out_flat.stride(1), out_flat.stride(2),
                start, chunk_len, D, BLOCK_D=FLASHRNN_BLOCK_D, CHUNK=chunk,
            )

        return out_flat.reshape(B, T, H, D), s_flat.reshape(B, H, D, D)

class AttentionWrapper(nn.Module):
    def __init__(self, qwen_layer, rwkv_layer, d_model):
        super().__init__()
        self.teacher_norm = qwen_layer.input_layernorm
        self.student_mixer = rwkv_layer
        self.d_model = d_model
        # We no longer hold the teacher_attn here to avoid running it twice.
        # We accept the TARGET output as input.
        for p in self.teacher_norm.parameters(): p.requires_grad = False

    def forward(self, hidden_states, teacher_target):
        # 1. Norm input
        normed_states = self.teacher_norm(hidden_states)
        
        # 2. Run Student
        student_out, _ = self.student_mixer(normed_states)
        
        # 3. Calc Loss against PRE-COMPUTED Teacher Target
        diff = teacher_target - student_out
        loss = torch.norm(diff, p=2, dim=-1).mean() * (self.d_model ** -0.5)
        return loss

class RWKV_Shim(nn.Module):
    def __init__(self, rwkv_module):
        super().__init__()
        self.rwkv = rwkv_module
    def forward(self, hidden_states, **kwargs):
        rwkv_out, new_state = self.rwkv(hidden_states)
        return (rwkv_out, None)

# ==============================================================================
# 4. TRAINING PIPELINE
# ==============================================================================
RWKV_HEAD_SIZE = 64

def build_arwkv_stage1_model(model_name="deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"):
    print(f"Loading Teacher Model: {model_name}")
    teacher_model = Qwen2ForCausalLM.from_pretrained(
        model_name, torch_dtype=torch.bfloat16, device_map="cuda", attn_implementation="eager"
    )
    config = teacher_model.config
    wrappers = nn.ModuleList()
    
    print("Grafting RWKV-7 Modules...")
    for i, layer in enumerate(teacher_model.model.layers):
        n_head_rwkv = config.hidden_size // RWKV_HEAD_SIZE
        rwkv_layer = RWKV7_TimeMix(
            layer_id=i, n_embd=config.hidden_size,
            n_head=n_head_rwkv, head_size=RWKV_HEAD_SIZE
        )
        rwkv_layer.to(torch.bfloat16).to("cuda")
        wrapper = AttentionWrapper(layer, rwkv_layer, config.hidden_size)
        wrappers.append(wrapper)
        
    return teacher_model, wrappers

def get_dataloader(tokenizer, batch_size=1):
    try:
        dataset = load_dataset("HuggingFaceFW/fineweb-edu", name="sample-10BT", split="train", streaming=True)
    except:
        dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, max_length=2048, padding="max_length")
    tokenized = dataset.map(tokenize_function, batched=True, remove_columns=["text"])
    return DataLoader(tokenized, batch_size=batch_size, collate_fn=default_data_collator)

# --- CAPTURE HOOK ---
class ActivationCapturer:
    def __init__(self):
        self.activations = {}
    def get_hook(self, layer_idx):
        def hook(module, args, output):
            # output of Qwen2Attention is (hidden_states, ...)
            self.activations[layer_idx] = output[0].detach()
        return hook

def train_stage_1(teacher_model, wrappers, dataloader, device="cuda", steps=100):
    print(f"Starting Stage 1 Alignment on {device}...")
    teacher_model.eval()
    
    # Register Hooks to capture Teacher Attention Output
    capturer = ActivationCapturer()
    handles = []
    for i, layer in enumerate(teacher_model.model.layers):
        handles.append(layer.self_attn.register_forward_hook(capturer.get_hook(i)))

    student_params = []
    for w in wrappers: student_params.extend(w.student_mixer.parameters())
    optimizer = optim.AdamW(student_params, lr=1e-3, betas=(0.9, 0.99))
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps * len(wrappers))
    
    progress_bar = tqdm(range(steps))
    data_iter = iter(dataloader)
    
    for step in progress_bar:
        try: batch = next(data_iter)
        except StopIteration: data_iter = iter(dataloader); batch = next(data_iter)
        input_ids = batch['input_ids'].to(device)
        mask = batch['attention_mask'].to(device)
        
        # 1. Run Full Teacher Forward Pass (Populates Hooks)
        with torch.no_grad():
            outputs = teacher_model(input_ids, output_hidden_states=True)
            # hidden_states tuple: (embedding_output, layer_0_out, layer_1_out, ...)
            # teacher_model.hidden_states contains INPUTS to each layer (roughly)
        
        # 2. Iterate Layers
        for layer_idx, wrapper in enumerate(wrappers):
            optimizer.zero_grad()
            
            # Input to Layer N is hidden_states[layer_idx] (after norm)
            # But wait, Qwen2DecoderLayer does Norm -> Attn.
            # Wrapper handles Norm. We just need the raw input state.
            # outputs.hidden_states[0] is embedding.
            # outputs.hidden_states[1] is output of layer 0.
            # Input to layer 0 is outputs.hidden_states[0].
            teacher_input = outputs.hidden_states[layer_idx]
            
            # Target is the captured output from the hook
            teacher_target = capturer.activations[layer_idx]
            
            loss = wrapper(teacher_input, teacher_target)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(wrapper.student_mixer.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            
            # Free memory
            del teacher_target
            
        capturer.activations.clear() # Reset for next batch
        
    for h in handles: h.remove()
    print("Stage 1 Training Complete.")
    return wrappers

# ==============================================================================
# 5. MODEL ASSEMBLY & STAGE 2
# ==============================================================================
def create_student_model(teacher_model, stage1_wrappers):
    print("Cloning Teacher to create Student Base...")
    student_model = copy.deepcopy(teacher_model)
    print("Surgically replacing Attention with RWKV-7...")
    for i, layer in enumerate(student_model.model.layers):
        trained_rwkv = stage1_wrappers[i].student_mixer
        layer.self_attn = RWKV_Shim(trained_rwkv)
    return student_model

def train_stage_2(teacher_model, student_model, dataloader, device="cuda", steps=100):
    print(f"Starting Stage 2 (Knowledge Distillation)...")
    teacher_model.eval(); student_model.train()
    
    params = [p for p in student_model.parameters()]
    optimizer = optim.AdamW(params, lr=5e-5)
    loss_fct = nn.KLDivLoss(reduction="batchmean")
    
    progress_bar = tqdm(range(steps))
    data_iter = iter(dataloader)
    
    for step in progress_bar:
        try: batch = next(data_iter)
        except StopIteration: data_iter = iter(dataloader); batch = next(data_iter)
        
        input_ids = batch['input_ids'].to(device)
        mask = batch['attention_mask'].to(device)
        
        optimizer.zero_grad()
        with torch.no_grad():
            t_logits = teacher_model(input_ids, attention_mask=mask).logits
            
        s_logits = student_model(input_ids, attention_mask=mask).logits
        
        loss = loss_fct(F.log_softmax(s_logits, dim=-1), F.softmax(t_logits, dim=-1))
        loss.backward()
        torch.nn.utils.clip_grad_norm_(params, 1.0)
        optimizer.step()
        progress_bar.set_description(f"KD Loss: {loss.item():.4f}")
    return student_model

# ==============================================================================
# 6. EXECUTION BLOCK
# ==============================================================================
if __name__ == "__main__":
    torch.cuda.empty_cache()
    model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    
    # Phase 1
    teacher, wrappers = build_arwkv_stage1_model(model_name)
    loader = get_dataloader(tokenizer, batch_size=1)
    train_stage_1(teacher, wrappers, loader, device="cuda", steps=1000)
    
    # Phase 2
    student = create_student_model(teacher, wrappers)
    trained_student = train_stage_2(teacher, student, loader, device="cuda", steps=1000)
    
    # Inference
    print("\n--- Testing ARWKV Model ---")
    trained_student.eval()
    trained_student.to(dtype=torch.float16, device="cuda")
    
    text = tokenizer.apply_chat_template(
        [{"role": "user", "content": "Explain the difference between a Transformer and an RNN."}],
        tokenize=False, add_generation_prompt=True
    )
    inputs = tokenizer([text], return_tensors="pt").to("cuda")
    
    streamer = TextStreamer(tokenizer)
    with torch.no_grad():
        trained_student.generate(
            **inputs, max_new_tokens=100, use_cache=False, streamer=streamer
        )
