# Faithful-replication-of-ARWKV
A faithful Replication of the "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer" paper, at a smaller scale (and done entirely within a Google Colab Notebook)
